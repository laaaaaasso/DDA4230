{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laaaaaasso/DDA4230/blob/main/Assignment_2_(student_copy).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Open AI Gym\n",
        "[OpenAI Gym](https://gym.openai.com/) aims to provide an easy-to-setup general-intelligence benchmark with a wide variety of different environments. The goal is to standardize how environments are defined in AI research publications so that published research becomes more easily reproducible. The project claims to provide the user with a simple interface.\n",
        "\n",
        "### Looking at Gym Environments\n",
        "\n",
        "The centerpiece of Gym is the environment, which defines the \"game\" in which your reinforcement algorithm will compete.  An environment does not need to be a game; however, it describes the following game-like features:\n",
        "* **action space**: What actions can we take on the environment, at each step/episode, to alter the environment.\n",
        "* **observation space**: What is the current state of the portion of the environment that we can observe. Usually, we can see the entire environment.\n",
        "\n",
        "Before we begin to look at Gym, it is essential to understand some of the terminology used by this library.\n",
        "\n",
        "* **Agent** - The machine learning program or model that controls the actions.\n",
        "Step - One round of issuing actions that affect the observation space.\n",
        "* **Episode** - A collection of steps that terminates when the agent fails to meet the environment's objective, or the episode reaches the maximum number of allowed steps.\n",
        "* **Render** - Gym can render one frame for display after each episode.\n",
        "* **Reward** - A positive reinforcement that can occur at the end of each episode, after the agent acts.\n",
        "* **Nondeterministic** - For some environments, randomness is a factor in deciding what effects actions have on reward and changes to the observation space.\n",
        "\n",
        "It is important to note that many of the gym environments specify that they are not nondeterministic even though they make use of random numbers to process actions. It is generally agreed upon (based on the gym GitHub issue tracker) that nondeterministic property means that a deterministic environment will still behave randomly even when given consistent seed value. The seed method of an environment can be used by the program to seed the random number generator for the environment.\n",
        "\n",
        "Altough Google Colab does not allow us to render each environment, you can take a look at the environment at [OpenAI Gym Environments](https://gym.openai.com/envs/#classic_control)\n",
        "\n",
        "The Gym library allows us to query some of these attributes from environments. We will begin by looking at the FrozenLake-v0 environment. The RL agent will control the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile."
      ],
      "metadata": {
        "id": "r2bzuwfRwlvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
        "\n",
        "query_environment(\"FrozenLake-v1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52bUeKeXxEN1",
        "outputId": "938af0c6-f89a-4830-e3f1-de138e65c734"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(4)\n",
            "Observation Space: Discrete(16)\n",
            "Max Episode Steps: 100\n",
            "Nondeterministic: False\n",
            "Reward Range: (0, 1)\n",
            "Reward Threshold: 0.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.12/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent moves through a $4 \\times 4$ gridworld, with states numbered as follows:\n",
        "```\n",
        "[[ 0  1  2  3]\n",
        " [ 4  5  6  7]\n",
        " [ 8  9 10 11]\n",
        " [12 13 14 15]]\n",
        "```\n",
        "Each state can be categorized into the following states\n",
        "```\n",
        "SFFF       (S: starting point, safe)\n",
        "FHFH       (F: frozen surface, safe)\n",
        "FFFH       (H: hole, fall to your doom)\n",
        "HFFG       (G: goal, where the frisbee is located)\n",
        "```\n",
        "The agent has 4 potential actions:\n",
        "```\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "```\n",
        "\n",
        "Thus, $\\mathcal{S}^+ = \\{0, 1, \\ldots, 15\\}$, and $\\mathcal{A} = \\{0, 1, 2, 3\\}$.  "
      ],
      "metadata": {
        "id": "usLMbZjFxZqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value Iteration\n",
        "\n",
        "Now we are going to instance a Fronzen lake environment, and try to implement Value itertaions.\n",
        "\n",
        "**You need to implement the code at where is labeled ``TODO:''**"
      ],
      "metadata": {
        "id": "qB2apmzV0VE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valueIteration(env, gamma=1.0, iteration=10000, eps=1e-20):\n",
        "    value = numpy.zeros(16)\n",
        "\n",
        "    # Start Iteration\n",
        "    for i in range(iteration):\n",
        "        for state in range(16):\n",
        "            Q = []\n",
        "            for action in range(4):\n",
        "                outcomes  = []\n",
        "\n",
        "                # TODO: Loop through all outcomes of performing that action and calculate the Q values\n",
        "                for probability, nextstate, reward, done in env.P[state][action]:\n",
        "                    None\n",
        "\n",
        "            # TODO: Calculate the value function\n",
        "            value[state] = None\n",
        "\n",
        "        if(numpy.sum(numpy.fabs(previous_value - value)) <= eps):\n",
        "            return value\n",
        "\n",
        "    print(\"Iterated over %d Iterations and couldn't converge\"%(iteration))\n",
        "    return value"
      ],
      "metadata": {
        "id": "WziUYeH21Kgi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculatePolicy(env, v, gamma=1.0):\n",
        "    policy = numpy.zeros(16)\n",
        "    for state in range(16):\n",
        "        Q = []\n",
        "        for action in range(4):\n",
        "\n",
        "            # Reset all outcomes of this particular action\n",
        "                outcomes  = []\n",
        "\n",
        "                # TODO: Loop through all outcomes of performing that action and calculate the Q values\n",
        "                for probability, nextstate, reward, done in env.P[state][action]:\n",
        "                    None\n",
        "\n",
        "        # TODO: extract policy\n",
        "        policy[state] = None\n",
        "\n",
        "    # Step 9: Return policy\n",
        "    return policy"
      ],
      "metadata": {
        "id": "1bhoY6iS1Z0m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluatePolicyOnce(env, policy, gamma=1.0, eps=1e-20):\n",
        "    totalreward = 0\n",
        "    step = 0\n",
        "    start = env.reset()\n",
        "\n",
        "    while True:\n",
        "        action = policy[start]\n",
        "\n",
        "        start, reward, done, _ = env.step(action)\n",
        "\n",
        "        # TODO: Calculate the total reward with the given discount factor\n",
        "        totalreward += None\n",
        "        step += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    return totalReward\n",
        "\n",
        "def evaluatePolicy(env, policy, gamma=1.0, iteration=100, render=False, eps=1e-20):\n",
        "    scores = [\n",
        "    evaluatePolicyOnce(env, policy, gamma, eps)\n",
        "        for _ in range(iteration)\n",
        "    ]\n",
        "\n",
        "    return numpy.mean(scores)"
      ],
      "metadata": {
        "id": "miYR0eCf1k65"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ValueIterator(env, gamma=1.0, improvementIterations=1000, evaluationIterations=100, eps=1e-20):\n",
        "    optimalValue = valueIteration(env, gamma, improvementIterations, eps)\n",
        "    policy = calculatePolicy(env, optimalValue, gamma)\n",
        "    score = evaluatePolicy(env, policy, gamma, evaluationIterations, eps)\n",
        "    print('Score obtained', score)\n",
        "    print('Best score = %0.2f.'%(numpy.max(score)))"
      ],
      "metadata": {
        "id": "YpmdwnGg0pAU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 1.0\n",
        "game = 'FrozenLake-v1'\n",
        "improvementIterations = 10000\n",
        "evaluationIterations = 100\n",
        "eps = 1e-20\n",
        "numpy.random.seed(0)\n",
        "env = gym.make(game)\n",
        "env.seed(0)\n",
        "\n",
        "env = gym.make(game).env\n",
        "ValueIterator(env=env, gamma=gamma, evaluationIterations=evaluationIterations, improvementIterations=improvementIterations, eps=eps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Zfl9dhA-002H",
        "outputId": "ad9e0471-2562-40f5-edf1-d021bda75b31"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'previous_value' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-576262501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mValueIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluationIterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluationIterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimprovementIterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimprovementIterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3512444817.py\u001b[0m in \u001b[0;36mValueIterator\u001b[0;34m(env, gamma, improvementIterations, evaluationIterations, eps)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mValueIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimprovementIterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluationIterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0moptimalValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalueIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimprovementIterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculatePolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimalValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluatePolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluationIterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Score obtained'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-607247005.py\u001b[0m in \u001b[0;36mvalueIteration\u001b[0;34m(env, gamma, iteration, eps)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_value\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'previous_value' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also provide the following code to visualize your calculated value function."
      ],
      "metadata": {
        "id": "a3uWayYwBP15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_values(V):\n",
        "\t# reshape value function\n",
        "\tV_sq = numpy.reshape(V, (4,4))\n",
        "\n",
        "\t# plot the state-value function\n",
        "\tfig = plt.figure(figsize=(6, 6))\n",
        "\tax = fig.add_subplot(111)\n",
        "\tim = ax.imshow(V_sq, cmap='cool')\n",
        "\tfor (j,i),label in numpy.ndenumerate(V_sq):\n",
        "\t    ax.text(i, j, numpy.round(label, 5), ha='center', va='center', fontsize=14)\n",
        "\tplt.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
        "\tplt.title('State-Value Function')\n",
        "\tplt.show()\n",
        "\n",
        "optimalValue = valueIteration(env, gamma, improvementIterations, eps)\n",
        "plot_values(optimalValue)"
      ],
      "metadata": {
        "id": "6CFNOBvxBXSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Iteration\n",
        "\n",
        "With the Frozen Lake environment, we are also going to try out the policy iteration method.\n",
        "\n",
        "**Again, please implement the code labeled #TODO**"
      ],
      "metadata": {
        "id": "kdRFSRaxCr4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Given a policy, extract its corresponding value function\"\"\"\n",
        "def get_value_function(env, policy, eps=1e-20):\n",
        "\n",
        "    value = numpy.zeros(16)\n",
        "\n",
        "    while True:\n",
        "        previous_value = numpy.copy(value)\n",
        "        for state in range(policy.size):\n",
        "            outcomes  = []\n",
        "\n",
        "            # TODO: Extract the action from policy\n",
        "            action = policy[state]\n",
        "\n",
        "            for probability, next_state, reward, is_done in env.P[state][action]:\n",
        "                # TODO: Get all outcomes of that action\n",
        "\n",
        "            # TODO: Extract the value function\n",
        "\n",
        "        if numpy.sum(numpy.fabs(previous_value - value)) <= eps:\n",
        "            return value\n",
        "\n",
        "    return value"
      ],
      "metadata": {
        "id": "1M8Q6C3BDvRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policyIteration(env, gamma=1.0, improvementIterations=1000, eps=1e-20):\n",
        "\n",
        "    # Declare a random policy\n",
        "    policy = numpy.random.choice(4, size=(16))\n",
        "\n",
        "    for i in range(improvementIterations):\n",
        "\n",
        "        # TODO: calculate the value function with the previous policy,\n",
        "        #  then use this value function to iterate for an improved pocliy\n",
        "        #  Hint: you've already implemented how to use a value function\n",
        "        #  to calculate a policy in calculatePolicy\n",
        "\n",
        "        # If converged\n",
        "        if(numpy.all(policy == newPolicy)):\n",
        "            return policy\n",
        "        policy = newPolicy\n",
        "\n",
        "    print(\"Iterated over %d Iterations and couldn't converge\"%(improvementIterations))\n",
        "    return policy\n"
      ],
      "metadata": {
        "id": "-M3mZxBEDGqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 1.0\n",
        "improvementIterations = 10000\n",
        "evaluationIterations = 100\n",
        "eps = 1e-20\n",
        "numpy.random.seed(0)\n",
        "env = gym.make(game)\n",
        "env.seed(0)\n",
        "\n",
        "def PolicyIterator(env, gamma=1.0, improvementIterations=1000, eps=1e-20, evaluationIterations=100):\n",
        "    optimalPolicy = policyIteration(env=env, gamma=gamma, improvementIterations=improvementIterations, eps=eps)\n",
        "    startTime = time.time()\n",
        "    policy_score = evaluatePolicy(env=env, policy=optimalPolicy, render=False, gamma=gamma, evaluationIterations=evaluationIterations, eps=eps)\n",
        "    endTime = time.time()\n",
        "    print('Best score = %0.2f. Time taken = %4.4f seconds'%(numpy.max(policy_score), endTime - startTime))"
      ],
      "metadata": {
        "id": "unSN-JxzDa6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualize the value function with respect to the policy."
      ],
      "metadata": {
        "id": "UyAVw8XNDyLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimalPolicy = policyIteration(env=env, gamma=gamma, improvementIterations=improvementIterations, eps=eps)\n",
        "PolicyValue = get_value_function(env=env, policy=optimalPolicy, eps=eps)\n",
        "\n",
        "plot_values(PolicyValue)"
      ],
      "metadata": {
        "id": "zjBUj9j7Dw7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "a047d49a-e471-46fc-fa87-34181cf71f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-de0227f16a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimalPolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicyIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimprovementIterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimprovementIterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mPolicyValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_value_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimalPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolicyValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-34707ca7260a>\u001b[0m in \u001b[0;36mpolicyIteration\u001b[0;34m(env, gamma, improvementIterations, eps)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# If converged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnewPolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'newPolicy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing the value and policy iteration methods.\n",
        "\n",
        "Lastly, please try to play around with the FrozenLake environment with different configurations (gamma, improvement/evaluation iterations, random seed)and with value and policy iteration. Please try your best to get a policy with score as high as possible (though your grades will not be affected by this score). Repeat each experiment with 5 different random seed, then report the highest score you've achieved across the 5 repeats with 1 standard deviation and write (a few sentence) to compare the two iteration methods."
      ],
      "metadata": {
        "id": "t9pXQQR_ENT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO:"
      ],
      "metadata": {
        "id": "KGXESDTEEQL4"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}